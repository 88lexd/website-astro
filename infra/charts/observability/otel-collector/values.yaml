---
# Using YAML Anchors to remind myself that the service is created as an extraObject and the service name is hard coded.
lbServiceName: &lbServiceNameAnchor otel-daemon-collector-lb
# If the collector name ever changes, make sure the Service LoadBalancer selector matches
lbServiceSelector: &lbServiceLabelSelector
  app.kubernetes.io/component: opentelemetry-collector
  app.kubernetes.io/instance: monitoring.otel-daemon
  app.kubernetes.io/managed-by: opentelemetry-operator
  app.kubernetes.io/part-of: opentelemetry

opentelemetry-operator:
  enabled: true

# The default configuration for all collectors generated by the chart.
# Any collectors in the `collectors` are overlayed on top of this configuration.
defaultCRConfig:
  # Annotations for the collector's pods
  podAnnotations:
    prometheus.io/scrape: "true"

# Collectors is a map of collector configurations of the form:
# collectors:
#   collectorName:
#     enabled: true
#     name: "example"
# Each collector configuration is layered on top of the `defaultCRConfig`, overriding a default if set.
collectors:
  daemon:
    enabled: true
    suffix: daemon
    mode: daemonset
    resources:
      requests:
        cpu: 50m
        memory: 250Mi
      limits:
        cpu: "1"
        memory: 500Mi

    # Target allocator configuration
    targetAllocator:
      enabled: true
      # image: myregistry/myimage:latest
      replicas: 1
      allocationStrategy: per-node
      resources:
        requests:
          cpu: "50m"
          memory: "64Mi"
        limits:
          cpu: "500m"
          memory: "128Mi"

    # Tolerations for the collector to also run as Daemon on the master node
    tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"

    # A scrape config file (part of the kube-stack Chart itself) to instruct the daemon collector to pull metrics from any matching targets on the same node with
    # prometheus.io/scrape=true
    scrape_configs_file: "daemon_scrape_configs.yaml"
    presets:
      logsCollection:
        enabled: true
        includeCollectorLogs: false
      kubeletMetrics:
        enabled: true
      hostMetrics:
        enabled: true
      kubernetesAttributes:
        enabled: true
      kubernetesEvents:
        enabled: false
      clusterMetrics:
        enabled: false

    # Observability configuration for the collector
    observability:
      metrics:
        enableMetrics: true

    # Environment variables for the collector
    env:
      - name: NEW_RELIC_API_KEY
        valueFrom:
          secretKeyRef:
            name: newrelic-api
            key: api-key

    config:
      #################
      # Start Recivers
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318
      # End Recivers


      ###################
      # Start Processors
      processors:
        batch:
          send_batch_size: 4096
          send_batch_max_size: 4096
          timeout: 5s

        resourcedetection/env:
          detectors: [env]
          timeout: 3s
          override: false

        # Keeping minimal logs locally.
        # Will be use NewRelic but retention there is much shorter.
        filter/include-logs:
          logs:
            # any logs NOT matching filters are excluded from remainder of pipeline
            include:
              match_type: regexp
              resource_attributes:
                - key: k8s.namespace.name
                  value: "(ingress-nginx|prod|dev)"

        resource/reduce-attributes:
          attributes:
            - key: k8s.cluster.uid
              action: delete
            - key: k8s.pod.uid
              action: delete
            - key: k8s.replicaset.uid
              action: delete

        filter/exclude-metrics:
          # any names NOT matching filters are excluded from remainder of pipeline
          metrics:
            include:
              match_type: regexp
              regexp:
                cacheenabled: true
              metric_names:
                - k8s\.node\.cpu\.usage
                - k8s\.node\.memory\.usage
                - k8s\.node\.uptime
                - k8s\.pod\.cpu\.usage
                - k8s\.pod\.memory\.usage
            # exclude:
            #   match_type: regexp
            #   regexp:
            #     cacheenabled: true
            #   metric_names:
            #     - system\.+
            #     - k8s\.node\.filesystem\.+
            #     - k8s\.node\.memory\.+
            #     - k8s\.pod\.filesystem\.+
            #     - k8s\.pod\.memory\.+
            #     - k8s\.pod\.volume\.+
            #     - k8s\.pod\.network\.+
            #     - k8s\.volume\.+
            #     - container\.cpu\.+
            #     - container\.memory\.+
            #     - container\.filesystem\.+

      # End Processors


      ##################
      # Start Exporters
      exporters:
        debug: {}

        otlphttp/victoriaLogs:
          # Note: Victoria Logs docs mentions the use of port `9428`, but this only applies to the `single` installation
          # I am running Victoria Logs Cluster, so the insert service uses port `9481` instead
          logs_endpoint: http://vlc-victoria-logs-cluster-vlinsert.monitoring.svc:9481/insert/opentelemetry/v1/logs


        otlphttp/newRelic:
          endpoint: https://otlp.nr-data.net:443
          headers:
            api-key: ${env:NEW_RELIC_API_KEY}
          compression: gzip
      # End Exporters

      ##########################
      # Start Service Pipelines
      service:
        pipelines:
          metrics:
            receivers:
              - prometheus # Created by preset
              - hostmetrics # Created by preset
              - otlp
            processors:
              # The k8sattributes processor is created by the preset. Validate by running:
              # $ kubectl get opentelemetrycollectors -n monitoring otel-daemon -o yaml | yq '.spec.config.processors.k8sattributes'
              - k8sattributes
              - resourcedetection/env
              - filter/exclude-metrics
              - batch
            exporters:
              - debug
              - otlphttp/newRelic

          logs/victoriaLogs:
            # This receiver is created by the preset. Validate by running:
            # $ kubectl get opentelemetrycollectors -n monitoring otel-daemon -o yaml | yq '.spec.config.receivers.filelog'
            receivers: [filelog]
            processors:
              # The k8sattributes processor is created by the preset (same as metrics)
              - k8sattributes
              - filter/include-logs
              - resourcedetection/env
              - resource/reduce-attributes
              - batch
            exporters:
              # - debug
              - otlphttp/victoriaLogs

          logs/newRelic:
            # This receiver is created by the preset (same as above)
            receivers: [filelog]
            processors:
              # The k8sattributes processor is created by the preset (same as metrics)
              - k8sattributes
              - resourcedetection/env
              - batch
            exporters:
              # - debug
              - otlphttp/newRelic

          logs/directOtlp:
            receivers: [otlp]
            processors:
              - batch
            exporters:
              - debug
              - otlphttp/newRelic
      # End Service Pipelines

extraObjects:
- apiVersion: v1
  kind: Service
  metadata:
    name: *lbServiceNameAnchor
  spec:
    type: LoadBalancer
    selector:
      <<: *lbServiceLabelSelector
    ports:
    - appProtocol: grpc
      name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - appProtocol: http
      name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
